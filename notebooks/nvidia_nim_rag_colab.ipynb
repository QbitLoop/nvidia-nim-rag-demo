{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ NVIDIA NIM RAG Demo\n",
        "\n",
        "**Build a RAG Pipeline with NVIDIA NIM API**\n",
        "\n",
        "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using NVIDIA NIM inference microservices.\n",
        "\n",
        "## What You'll Learn\n",
        "- Using NVIDIA NIM API for LLM inference\n",
        "- Creating embeddings with NV-Embed-QA\n",
        "- Building a simple RAG pipeline\n",
        "- Measuring latency and performance\n",
        "\n",
        "**Requirements**: NVIDIA API key from [build.nvidia.com](https://build.nvidia.com)\n",
        "\n",
        "---\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QbitLoop/nvidia-nim-rag-demo/blob/main/notebooks/nvidia_nim_rag_colab.ipynb)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-QbitLoop-blue)](https://github.com/QbitLoop/nvidia-nim-rag-demo)"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Setup & Dependencies"
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q openai numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get API key from Colab secrets\n",
        "# Add your NVIDIA_API_KEY in Colab: Runtime > Secrets\n",
        "try:\n",
        "    NVIDIA_API_KEY = userdata.get('NVIDIA_API_KEY')\n",
        "except:\n",
        "    NVIDIA_API_KEY = input(\"Enter your NVIDIA API key: \")\n",
        "\n",
        "print(\"‚úÖ API key configured\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Initialize NVIDIA NIM Client\n",
        "\n",
        "NVIDIA NIM uses an OpenAI-compatible API, making it easy to integrate."
      ],
      "metadata": {
        "id": "nim-client-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize NIM client (OpenAI-compatible)\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=NVIDIA_API_KEY\n",
        ")\n",
        "\n",
        "# Model configuration\n",
        "LLM_MODEL = \"meta/llama-3.1-70b-instruct\"\n",
        "EMBED_MODEL = \"nvidia/nv-embedqa-e5-v5\"\n",
        "\n",
        "print(f\"‚úÖ NIM client initialized\")\n",
        "print(f\"   LLM: {LLM_MODEL}\")\n",
        "print(f\"   Embeddings: {EMBED_MODEL}\")"
      ],
      "metadata": {
        "id": "nim-client"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Test LLM Inference\n",
        "\n",
        "Let's test the NIM LLM endpoint and measure latency."
      ],
      "metadata": {
        "id": "llm-test-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_nim(prompt: str, max_tokens: int = 256) -> tuple[str, float]:\n",
        "    \"\"\"Send a prompt to NIM and return response with latency.\"\"\"\n",
        "    start = time.time()\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=LLM_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    latency_ms = (time.time() - start) * 1000\n",
        "    return response.choices[0].message.content, latency_ms\n",
        "\n",
        "# Test the LLM\n",
        "response, latency = chat_with_nim(\"What is NVIDIA NIM in one sentence?\")\n",
        "\n",
        "print(f\"üìù Response: {response}\")\n",
        "print(f\"\\n‚è±Ô∏è Latency: {latency:.0f}ms\")"
      ],
      "metadata": {
        "id": "llm-test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Create Embeddings\n",
        "\n",
        "NVIDIA's NV-Embed-QA model provides high-quality embeddings for RAG applications."
      ],
      "metadata": {
        "id": "embed-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text: str, input_type: str = \"passage\") -> tuple[list, float]:\n",
        "    \"\"\"Get embedding for text using NVIDIA NV-Embed-QA.\"\"\"\n",
        "    start = time.time()\n",
        "    \n",
        "    response = client.embeddings.create(\n",
        "        model=EMBED_MODEL,\n",
        "        input=[text],\n",
        "        extra_body={\"input_type\": input_type, \"truncate\": \"END\"}\n",
        "    )\n",
        "    \n",
        "    latency_ms = (time.time() - start) * 1000\n",
        "    return response.data[0].embedding, latency_ms\n",
        "\n",
        "# Test embedding\n",
        "test_text = \"NVIDIA NIM is an inference microservice for deploying AI models.\"\n",
        "embedding, latency = get_embedding(test_text)\n",
        "\n",
        "print(f\"üìä Embedding dimension: {len(embedding)}\")\n",
        "print(f\"‚è±Ô∏è Latency: {latency:.0f}ms\")\n",
        "print(f\"\\nüìà First 5 values: {embedding[:5]}\")"
      ],
      "metadata": {
        "id": "embeddings"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Build Simple RAG Pipeline\n",
        "\n",
        "Now let's build a complete RAG system with:\n",
        "- Document storage (in-memory)\n",
        "- Semantic search\n",
        "- Augmented generation"
      ],
      "metadata": {
        "id": "rag-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRAG:\n",
        "    \"\"\"Simple RAG implementation using NVIDIA NIM.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.documents = []\n",
        "        self.embeddings = []\n",
        "    \n",
        "    def add_documents(self, docs: list[str]):\n",
        "        \"\"\"Add documents to the knowledge base.\"\"\"\n",
        "        print(f\"üìö Adding {len(docs)} documents...\")\n",
        "        for doc in docs:\n",
        "            embedding, _ = get_embedding(doc, input_type=\"passage\")\n",
        "            self.documents.append(doc)\n",
        "            self.embeddings.append(embedding)\n",
        "        print(f\"‚úÖ {len(self.documents)} documents indexed\")\n",
        "    \n",
        "    def search(self, query: str, top_k: int = 3) -> list[tuple[str, float]]:\n",
        "        \"\"\"Search for relevant documents.\"\"\"\n",
        "        # Get query embedding (use \"query\" type for asymmetric search)\n",
        "        query_embedding, _ = get_embedding(query, input_type=\"query\")\n",
        "        \n",
        "        # Compute cosine similarity\n",
        "        query_np = np.array(query_embedding)\n",
        "        scores = []\n",
        "        for doc_emb in self.embeddings:\n",
        "            doc_np = np.array(doc_emb)\n",
        "            similarity = np.dot(query_np, doc_np) / (np.linalg.norm(query_np) * np.linalg.norm(doc_np))\n",
        "            scores.append(similarity)\n",
        "        \n",
        "        # Get top-k\n",
        "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "        return [(self.documents[i], scores[i]) for i in top_indices]\n",
        "    \n",
        "    def query(self, question: str) -> tuple[str, list, float]:\n",
        "        \"\"\"Run RAG query: retrieve + generate.\"\"\"\n",
        "        start = time.time()\n",
        "        \n",
        "        # Retrieve relevant documents\n",
        "        relevant_docs = self.search(question, top_k=3)\n",
        "        \n",
        "        # Build context\n",
        "        context = \"\\n\\n\".join([f\"[{i+1}] {doc}\" for i, (doc, _) in enumerate(relevant_docs)])\n",
        "        \n",
        "        # Generate response with context\n",
        "        prompt = f\"\"\"Based on the following context, answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (cite sources using [1], [2], etc.):\"\"\"\n",
        "        \n",
        "        response, _ = chat_with_nim(prompt, max_tokens=512)\n",
        "        \n",
        "        total_latency = (time.time() - start) * 1000\n",
        "        return response, relevant_docs, total_latency\n",
        "\n",
        "# Initialize RAG\n",
        "rag = SimpleRAG()"
      ],
      "metadata": {
        "id": "rag-class"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add sample documents about NVIDIA\n",
        "nvidia_docs = [\n",
        "    \"NVIDIA NIM is an inference microservice that enables easy deployment of AI models with optimized performance. It provides pre-built containers for popular models.\",\n",
        "    \"NVIDIA DGX Cloud offers AI supercomputing in the cloud, providing access to NVIDIA's latest hardware like H100 GPUs for training and inference workloads.\",\n",
        "    \"NVIDIA Nemotron is a family of large language models trained by NVIDIA, optimized for enterprise use cases and available through NIM.\",\n",
        "    \"NVIDIA TensorRT is a high-performance deep learning inference optimizer and runtime that delivers low latency and high throughput for production deployments.\",\n",
        "    \"NVIDIA CUDA is a parallel computing platform that enables developers to use NVIDIA GPUs for general-purpose computing, dramatically accelerating AI workloads.\",\n",
        "    \"NVIDIA Triton Inference Server is an open-source software that simplifies deployment of AI models at scale, supporting multiple frameworks and model formats.\"\n",
        "]\n",
        "\n",
        "rag.add_documents(nvidia_docs)"
      ],
      "metadata": {
        "id": "add-docs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Test RAG Queries"
      ],
      "metadata": {
        "id": "test-rag-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test RAG query\n",
        "question = \"What is NVIDIA NIM and how does it help with AI deployment?\"\n",
        "\n",
        "response, sources, latency = rag.query(question)\n",
        "\n",
        "print(f\"‚ùì Question: {question}\")\n",
        "print(f\"\\nüí¨ Answer:\\n{response}\")\n",
        "print(f\"\\nüìö Sources:\")\n",
        "for i, (doc, score) in enumerate(sources, 1):\n",
        "    print(f\"   [{i}] (score: {score:.3f}) {doc[:80]}...\")\n",
        "print(f\"\\n‚è±Ô∏è Total latency: {latency:.0f}ms\")"
      ],
      "metadata": {
        "id": "test-rag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try another query\n",
        "question = \"How can I optimize AI inference performance with NVIDIA?\"\n",
        "\n",
        "response, sources, latency = rag.query(question)\n",
        "\n",
        "print(f\"‚ùì Question: {question}\")\n",
        "print(f\"\\nüí¨ Answer:\\n{response}\")\n",
        "print(f\"\\n‚è±Ô∏è Total latency: {latency:.0f}ms\")"
      ],
      "metadata": {
        "id": "test-rag-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Performance Summary"
      ],
      "metadata": {
        "id": "summary-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmark multiple queries\n",
        "test_queries = [\n",
        "    \"What is NVIDIA NIM?\",\n",
        "    \"How do I deploy models with NVIDIA?\",\n",
        "    \"What hardware does NVIDIA offer for AI?\"\n",
        "]\n",
        "\n",
        "latencies = []\n",
        "for q in test_queries:\n",
        "    _, _, lat = rag.query(q)\n",
        "    latencies.append(lat)\n",
        "    print(f\"‚úÖ '{q[:40]}...' - {lat:.0f}ms\")\n",
        "\n",
        "print(f\"\\nüìä Performance Summary:\")\n",
        "print(f\"   Avg Latency: {np.mean(latencies):.0f}ms\")\n",
        "print(f\"   Min Latency: {np.min(latencies):.0f}ms\")\n",
        "print(f\"   Max Latency: {np.max(latencies):.0f}ms\")"
      ],
      "metadata": {
        "id": "benchmark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üéâ Summary\n",
        "\n",
        "You've built a RAG pipeline using NVIDIA NIM with:\n",
        "\n",
        "| Component | Technology |\n",
        "|-----------|------------|\n",
        "| **LLM** | Llama 3.1 70B via NIM |\n",
        "| **Embeddings** | NV-Embed-QA E5 v5 |\n",
        "| **Vector Store** | In-memory (numpy) |\n",
        "| **Search** | Cosine similarity |\n",
        "\n",
        "### Next Steps\n",
        "- Add persistent vector storage (pgvector, FAISS)\n",
        "- Implement document chunking\n",
        "- Add citation tracking\n",
        "- Deploy with FastAPI\n",
        "\n",
        "### Resources\n",
        "- [NVIDIA NIM Documentation](https://docs.nvidia.com/nim/)\n",
        "- [Full Demo Repo](https://github.com/QbitLoop/nvidia-nim-rag-demo)\n",
        "- [NVIDIA Build](https://build.nvidia.com)\n",
        "\n",
        "---\n",
        "\n",
        "*Built by [QbitLoop](https://github.com/QbitLoop) | MIT License*"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
